{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICCS 2022 - Models and Metrics for Mining Meaningful Metadata\n",
    "\n",
    "This notebook contains the implementation of Scheduler code for the paper \"Models and Metrics for Mining Meaningful Metadata\". The scheduler gets inputted file type identification vectors corresponding to a file and also \"model piles\" which are regressions that predict a file's expected extraction time and extracted metadata size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the Neccessary Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from math import log\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation of the Scheduler in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "\n",
    "\tdef __init__(self, models_dir):\n",
    "\t\tself.model_piles = dict()\n",
    "\t\tfor subdir, dirs, files in os.walk(models_dir):\n",
    "\t\t\tfor file_name in files:\t\t\n",
    "\t\t\t\textractor_name = file_name.split(\"_\")[2].split(\".\")[0]\n",
    "\t\t\t\tfile_path = os.path.join(subdir, file_name)\n",
    "\t\t\t\twith open(file_path, \"rb\") as f:\n",
    "\t\t\t\t\tself.model_piles[extractor_name] = pickle.load(f)\n",
    "\n",
    "\n",
    "\tdef run(self, prob_vectors, file_sizes):\n",
    "\t\tpriority_list = []\n",
    "\t\terror_files = set()\n",
    "\n",
    "\t\tfile_sizes_dict = self.parse_file_sizes(file_sizes)\n",
    "\n",
    "\t\twith open(prob_vectors, \"r\") as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\t\t\tcount = 0\n",
    "\t\t\tpairs_processed = 0\n",
    "\t\t\tfor name, vector in data.items():\n",
    "\t\t\t\tif count % 8000 == 0:\n",
    "\t\t\t\t\tprint(\"Curr count:\", count)\n",
    "\n",
    "\t\t\t\textrac_count = 0\n",
    "\t\t\t\tfor extractor, probability in vector['probabilities'].items():\n",
    "\t\t\t\t\tif extractor in self.model_piles:\n",
    "\t\t\t\t\t\texpected_size = None\n",
    "\t\t\t\t\t\texpected_time = None\n",
    "\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\ttime_model = self.model_piles[extractor]['extraction_time']\n",
    "\t\t\t\t\t\tif isinstance(time_model, float):\n",
    "\t\t\t\t\t\t\texpected_time = time_model\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\texpected_time = time_model.predict(file_sizes_dict[name])[0]\n",
    "\t\t\t\t\t\t\texcept KeyError as e:\n",
    "\t\t\t\t\t\t\t\tprint(e)\n",
    "\t\t\t\t\t\t\t\terror_files.add((name, count))\n",
    "\t\t\t\t\t\tsize_model = self.model_piles[extractor]['extraction_size']\n",
    "\n",
    "\t\t\t\t\t\tif isinstance(size_model, float):\n",
    "\t\t\t\t\t\t\texpected_size = size_model\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\texpected_size = size_model.predict(file_sizes_dict[name])[0]\n",
    "\n",
    "\t\t\t\t\t\tif expected_time != None and expected_size != None:\n",
    "\t\t\t\t\t\t\tpriority_value = self.calculate_benefit(probability, expected_time, expected_size)\n",
    "\t\t\t\t\t\t\tpriority_list.append((name, extractor, priority_value))\n",
    "\t\t\t\t\t\t\tpairs_processed += 1\n",
    "\n",
    "\t\t\t\t\t\t\textrac_count += 1\n",
    "\n",
    "\t\t\t\tcount += 1\n",
    "\t\tprint(\"Files processed: \", count)\n",
    "\t\tprint(\"Pairs Processed:\", pairs_processed)\n",
    "\n",
    "\t\treturn priority_list, list(error_files)\n",
    "\t\n",
    "\t# Was called calculating the cost but now we want to frame it as calculating the benefit\n",
    "\t# our objective function\n",
    "\tdef calculate_benefit(self, probability, expected_time, expected_size):\n",
    "\t\tsizes_probability = expected_size * probability + 1 # Laplacian Smoothing\n",
    "\t\tbenefit_raw = sizes_probability / (expected_time + np.finfo(float).eps + 1) # so we don't divide by zero\n",
    "\t\treturn -1 * log(benefit_raw) # priority sorts in increasing order so we flip it around \n",
    "\n",
    "\tdef parse_file_sizes(self, file_sizes):\n",
    "\t\tfile_sizes_dict = dict()\n",
    "\n",
    "\t\twith open(file_sizes, \"r\") as f:\n",
    "\t\t\tcsv_reader = csv.DictReader(f)\n",
    "\t\t\tfor row in csv_reader:\n",
    "\t\t\t\tfile_sizes_dict[row[\"path\"]] = np.array([row[\"size\"]]).reshape(1, -1)\n",
    "\n",
    "\t\treturn file_sizes_dict\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Example Test Run of the Scheduler in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = Scheduler(os.path.abspath(\"cdiac_model_piles/\"))\n",
    "output, error_files = scheduler.run(\"cdiac_probability_predictions.json\", \"csv-try-2.csv\")\n",
    "\n",
    "print(\"Error files: \", len(error_files))\n",
    "\n",
    "with open('cdiac_priority_list_3.pkl', 'wb+') as out:\n",
    "\tpickle.dump(output, out)\n",
    "\n",
    "with open('cdiac_error_files_3.pkl', 'wb+') as out:\n",
    "\tpickle.dump(error_files, out)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
